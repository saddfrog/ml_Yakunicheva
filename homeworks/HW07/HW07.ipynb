{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02308731",
   "metadata": {},
   "source": [
    "# HW07 – Кластеризация на синтетических данных\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54145b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score, adjusted_rand_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Настройка отображения графиков\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Константы\n",
    "RANDOM_STATE = 42\n",
    "DATA_PATH = \"./data/\"\n",
    "ARTIFACTS_PATH = \"./artifacts/\"\n",
    "FIGURES_PATH = \"./artifacts/figures/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4727cc",
   "metadata": {},
   "source": [
    "# 1. ФУНКЦИИ ДЛЯ АНАЛИЗА\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc68aef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filename):\n",
    "    \"\"\"Загрузка датасета из data/\"\"\"\n",
    "    path = os.path.join(DATA_PATH, filename)\n",
    "    df = pd.read_csv(path)\n",
    "    print(f\"Загружен {filename}: {df.shape[0]} строк, {df.shape[1]} столбцов\")\n",
    "    return df\n",
    "\n",
    "def basic_eda(df, dataset_name):\n",
    "    \"\"\"Базовый анализ датасета\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"АНАЛИЗ ДАТАСЕТА: {dataset_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    print(\"\\nПервые 5 строк:\")\n",
    "    display(df.head())\n",
    "    \n",
    "    print(\"\\nИнформация о типах данных:\")\n",
    "    df.info()\n",
    "    \n",
    "    print(\"\\nБазовые статистики:\")\n",
    "    display(df.describe())\n",
    "    \n",
    "    print(\"\\nПропуски данных:\")\n",
    "    missing = df.isnull().sum()\n",
    "    print(missing[missing > 0] if missing.any() else \"Пропусков нет\")\n",
    "    \n",
    "    print(f\"\\nУникальные значения в sample_id: {df['sample_id'].nunique()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def preprocess_data(df, dataset_name):\n",
    "    \"\"\"\n",
    "    Препроцессинг данных\n",
    "    Возвращает:\n",
    "    - X_scaled: масштабированные признаки\n",
    "    - X_raw: исходные признаки (без sample_id)\n",
    "    - sample_ids\n",
    "    \"\"\"\n",
    "    # Сохраняем sample_id отдельно\n",
    "    sample_ids = df['sample_id'].copy()\n",
    "    \n",
    "    # Признаки (все кроме sample_id)\n",
    "    X_raw = df.drop('sample_id', axis=1)\n",
    "    \n",
    "    # Проверяем типы признаков\n",
    "    numeric_cols = X_raw.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_cols = X_raw.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    print(f\"\\nПрепроцессинг {dataset_name}:\")\n",
    "    print(f\"  Числовые признаки: {len(numeric_cols)}\")\n",
    "    print(f\"  Категориальные признаки: {len(categorical_cols)}\")\n",
    "    \n",
    "    # Для HW07 все признаки числовые, но оставим общий случай\n",
    "    if len(categorical_cols) > 0:\n",
    "        print(\"  ВНИМАНИЕ: Есть категориальные признаки! Требуется кодирование.\")\n",
    "        # Здесь можно добавить OneHotEncoder\n",
    "        from sklearn.preprocessing import OneHotEncoder\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', StandardScaler(), numeric_cols),\n",
    "                ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "            ])\n",
    "    else:\n",
    "        # Только масштабирование для числовых признаков\n",
    "        preprocessor = StandardScaler()\n",
    "    \n",
    "    # Применяем препроцессинг\n",
    "    X_scaled = preprocessor.fit_transform(X_raw)\n",
    "    \n",
    "    # Если это был ColumnTransformer, преобразуем в массив\n",
    "    if hasattr(X_scaled, 'toarray'):\n",
    "        X_scaled = X_scaled.toarray()\n",
    "    \n",
    "    print(f\"  Размер после препроцессинга: {X_scaled.shape}\")\n",
    "    \n",
    "    return X_scaled, X_raw, sample_ids\n",
    "\n",
    "def evaluate_clustering(X, labels, dataset_name, model_name):\n",
    "    \"\"\"Оценка качества кластеризации\"\"\"\n",
    "    # Исключаем шумные точки для DBSCAN (-1)\n",
    "    if model_name == 'DBSCAN':\n",
    "        non_noise_mask = labels != -1\n",
    "        if non_noise_mask.sum() > 1:  # Нужно хотя бы 2 точки для метрик\n",
    "            X_eval = X[non_noise_mask]\n",
    "            labels_eval = labels[non_noise_mask]\n",
    "            noise_ratio = (labels == -1).mean()\n",
    "        else:\n",
    "            print(\"  ВНИМАНИЕ: Все точки помечены как шум!\")\n",
    "            return {\n",
    "                'silhouette': np.nan,\n",
    "                'davies_bouldin': np.nan,\n",
    "                'calinski_harabasz': np.nan,\n",
    "                'noise_ratio': 1.0\n",
    "            }\n",
    "    else:\n",
    "        X_eval = X\n",
    "        labels_eval = labels\n",
    "        noise_ratio = 0.0\n",
    "    \n",
    "    # Считаем метрики\n",
    "    n_clusters = len(np.unique(labels_eval))\n",
    "    \n",
    "    if n_clusters > 1:\n",
    "        silhouette = silhouette_score(X_eval, labels_eval)\n",
    "        db = davies_bouldin_score(X_eval, labels_eval)\n",
    "        ch = calinski_harabasz_score(X_eval, labels_eval)\n",
    "    else:\n",
    "        silhouette = np.nan\n",
    "        db = np.nan\n",
    "        ch = np.nan\n",
    "    \n",
    "    metrics = {\n",
    "        'silhouette': float(silhouette) if not np.isnan(silhouette) else None,\n",
    "        'davies_bouldin': float(db) if not np.isnan(db) else None,\n",
    "        'calinski_harabasz': float(ch) if not np.isnan(ch) else None,\n",
    "        'n_clusters': int(n_clusters),\n",
    "        'noise_ratio': float(noise_ratio) if model_name == 'DBSCAN' else 0.0\n",
    "    }\n",
    "    \n",
    "    print(f\"  Метрики для {model_name}:\")\n",
    "    print(f\"    Кластеров: {metrics['n_clusters']}\")\n",
    "    if model_name == 'DBSCAN':\n",
    "        print(f\"    Доля шума: {metrics['noise_ratio']:.2%}\")\n",
    "    print(f\"    Silhouette: {metrics['silhouette']:.3f}\" if metrics['silhouette'] else \"    Silhouette: N/A\")\n",
    "    print(f\"    Davies-Bouldin: {metrics['davies_bouldin']:.3f}\" if metrics['davies_bouldin'] else \"    Davies-Bouldin: N/A\")\n",
    "    print(f\"    Calinski-Harabasz: {metrics['calinski_harabasz']:.3f}\" if metrics['calinski_harabasz'] else \"    Calinski-Harabasz: N/A\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def visualize_clusters_pca(X, labels, dataset_name, model_name, params, save=False):\n",
    "    \"\"\"Визуализация кластеров в 2D PCA\"\"\"\n",
    "    # PCA для визуализации\n",
    "    pca = PCA(n_components=2, random_state=RANDOM_STATE)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    \n",
    "    # Создаем DataFrame для удобства\n",
    "    df_viz = pd.DataFrame({\n",
    "        'PC1': X_pca[:, 0],\n",
    "        'PC2': X_pca[:, 1],\n",
    "        'cluster': labels\n",
    "    })\n",
    "    \n",
    "    # Разделяем шум и кластеры для DBSCAN\n",
    "    if model_name == 'DBSCAN':\n",
    "        noise_mask = df_viz['cluster'] == -1\n",
    "        df_clusters = df_viz[~noise_mask]\n",
    "        df_noise = df_viz[noise_mask]\n",
    "    else:\n",
    "        df_clusters = df_viz\n",
    "        df_noise = None\n",
    "    \n",
    "    # Визуализация\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Рисуем кластеры\n",
    "    scatter = plt.scatter(df_clusters['PC1'], df_clusters['PC2'], \n",
    "                          c=df_clusters['cluster'], cmap='tab20', \n",
    "                          s=50, alpha=0.8, edgecolors='k', linewidth=0.5)\n",
    "    \n",
    "    # Рисуем шум отдельно (для DBSCAN)\n",
    "    if df_noise is not None and len(df_noise) > 0:\n",
    "        plt.scatter(df_noise['PC1'], df_noise['PC2'], \n",
    "                   c='gray', s=30, alpha=0.3, marker='x', \n",
    "                   label='Noise (cluster -1)')\n",
    "    \n",
    "    # Настройки графика\n",
    "    plt.colorbar(scatter, label='Cluster')\n",
    "    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "    \n",
    "    title = f'{dataset_name} - {model_name}'\n",
    "    if params:\n",
    "        title += f'\\n{params}'\n",
    "    plt.title(title)\n",
    "    \n",
    "    if df_noise is not None and len(df_noise) > 0:\n",
    "        plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save:\n",
    "        filename = f\"{dataset_name}_{model_name}_pca.png\".replace(' ', '_').lower()\n",
    "        filepath = os.path.join(FIGURES_PATH, filename)\n",
    "        plt.savefig(filepath, dpi=150, bbox_inches='tight')\n",
    "        print(f\"  График сохранен: {filepath}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Возвращаем объясненную дисперсию\n",
    "    return pca.explained_variance_ratio_.sum()\n",
    "\n",
    "def plot_silhouette_vs_k(X, dataset_name, k_range=(2, 20), save=False):\n",
    "    \"\"\"График silhouette vs k для KMeans\"\"\"\n",
    "    silhouette_scores = []\n",
    "    \n",
    "    for k in range(k_range[0], k_range[1]+1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=RANDOM_STATE, n_init=10)\n",
    "        labels = kmeans.fit_predict(X)\n",
    "        \n",
    "        if len(np.unique(labels)) > 1:\n",
    "            score = silhouette_score(X, labels)\n",
    "        else:\n",
    "            score = 0\n",
    "        silhouette_scores.append(score)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(k_range[0], k_range[1]+1), silhouette_scores, 'bo-')\n",
    "    plt.xlabel('Number of clusters (k)')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.title(f'{dataset_name} - Silhouette Score vs k for KMeans')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Отмечаем лучший k\n",
    "    best_k = range(k_range[0], k_range[1]+1)[np.argmax(silhouette_scores)]\n",
    "    plt.axvline(x=best_k, color='r', linestyle='--', alpha=0.7, \n",
    "                label=f'Best k = {best_k}')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save:\n",
    "        filename = f\"{dataset_name}_silhouette_vs_k.png\".replace(' ', '_').lower()\n",
    "        filepath = os.path.join(FIGURES_PATH, filename)\n",
    "        plt.savefig(filepath, dpi=150, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return best_k, max(silhouette_scores)\n",
    "\n",
    "def save_results(dataset_name, model_name, params, metrics, labels, sample_ids):\n",
    "    \"\"\"Сохранение результатов\"\"\"\n",
    "    # Сохраняем метки кластеров\n",
    "    results_df = pd.DataFrame({\n",
    "        'sample_id': sample_ids,\n",
    "        'cluster_label': labels\n",
    "    })\n",
    "    \n",
    "    filename = f\"labels_{dataset_name}_{model_name}.csv\".replace(' ', '_').lower()\n",
    "    filepath = os.path.join(ARTIFACTS_PATH, 'labels', filename)\n",
    "    \n",
    "    # Создаем директорию если нужно\n",
    "    Path(os.path.join(ARTIFACTS_PATH, 'labels')).mkdir(exist_ok=True)\n",
    "    \n",
    "    results_df.to_csv(filepath, index=False)\n",
    "    print(f\"  Метки кластеров сохранены: {filepath}\")\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5e7c83",
   "metadata": {},
   "source": [
    "# 2. АНАЛИЗ ДАТАСЕТА 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1250d553",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ДАТАСЕТ 01: Числовые признаки в разных шкалах + шумовые признаки\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Загрузка данных\n",
    "df1 = load_dataset(\"S07-hw-dataset-01.csv\")\n",
    "df1 = basic_eda(df1, \"Dataset 01\")\n",
    "\n",
    "# Препроцессинг\n",
    "X1_scaled, X1_raw, sample_ids1 = preprocess_data(df1, \"Dataset 01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8b545b",
   "metadata": {},
   "source": [
    "# 2.1 KMeans для Dataset 01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24d1935",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KMEANS для Dataset 01\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Подбор оптимального k\n",
    "best_k1, best_sil1 = plot_silhouette_vs_k(X1_scaled, \"Dataset 01\", k_range=(2, 15), save=True)\n",
    "\n",
    "# Лучшая модель KMeans\n",
    "kmeans1 = KMeans(n_clusters=best_k1, random_state=RANDOM_STATE, n_init=10)\n",
    "labels_kmeans1 = kmeans1.fit_predict(X1_scaled)\n",
    "\n",
    "# Оценка\n",
    "metrics_kmeans1 = evaluate_clustering(X1_scaled, labels_kmeans1, \"Dataset 01\", \"KMeans\")\n",
    "\n",
    "# Визуализация\n",
    "var_exp1_kmeans = visualize_clusters_pca(X1_scaled, labels_kmeans1, \n",
    "                                        \"Dataset 01\", \"KMeans\", \n",
    "                                        f\"k={best_k1}\", save=True)\n",
    "\n",
    "# Сохранение результатов\n",
    "save_results(\"dataset01\", \"kmeans\", \n",
    "            {\"n_clusters\": best_k1, \"random_state\": RANDOM_STATE},\n",
    "            metrics_kmeans1, labels_kmeans1, sample_ids1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc17163",
   "metadata": {},
   "source": [
    "# 2.2 DBSCAN для Dataset 01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed189f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DBSCAN для Dataset 01\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Подбор параметров DBSCAN\n",
    "eps_values = [0.3, 0.5, 0.7, 1.0, 1.5, 2.0]\n",
    "min_samples_values = [3, 5, 7, 10]\n",
    "\n",
    "best_eps1 = None\n",
    "best_min_samples1 = None\n",
    "best_silhouette1_db = -1\n",
    "best_labels1_db = None\n",
    "\n",
    "for eps in eps_values:\n",
    "    for min_samples in min_samples_values:\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        labels = dbscan.fit_predict(X1_scaled)\n",
    "        \n",
    "        # Исключаем шум для вычисления метрик\n",
    "        non_noise_mask = labels != -1\n",
    "        if non_noise_mask.sum() > 1 and len(np.unique(labels[non_noise_mask])) > 1:\n",
    "            silhouette = silhouette_score(X1_scaled[non_noise_mask], labels[non_noise_mask])\n",
    "        else:\n",
    "            silhouette = -1\n",
    "        \n",
    "        if silhouette > best_silhouette1_db:\n",
    "            best_silhouette1_db = silhouette\n",
    "            best_eps1 = eps\n",
    "            best_min_samples1 = min_samples\n",
    "            best_labels1_db = labels\n",
    "\n",
    "print(f\"Лучшие параметры DBSCAN: eps={best_eps1}, min_samples={best_min_samples1}\")\n",
    "print(f\"Лучший silhouette: {best_silhouette1_db:.3f}\")\n",
    "\n",
    "# Оценка лучшей модели DBSCAN\n",
    "dbscan1 = DBSCAN(eps=best_eps1, min_samples=best_min_samples1)\n",
    "labels_dbscan1 = dbscan1.fit_predict(X1_scaled)\n",
    "\n",
    "metrics_dbscan1 = evaluate_clustering(X1_scaled, labels_dbscan1, \"Dataset 01\", \"DBSCAN\")\n",
    "\n",
    "# Визуализация\n",
    "var_exp1_dbscan = visualize_clusters_pca(X1_scaled, labels_dbscan1, \n",
    "                                        \"Dataset 01\", \"DBSCAN\", \n",
    "                                        f\"eps={best_eps1}, min_samples={best_min_samples1}\", \n",
    "                                        save=True)\n",
    "\n",
    "# Сохранение результатов\n",
    "save_results(\"dataset01\", \"dbscan\", \n",
    "            {\"eps\": best_eps1, \"min_samples\": best_min_samples1},\n",
    "            metrics_dbscan1, labels_dbscan1, sample_ids1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5171e2",
   "metadata": {},
   "source": [
    "# 3. АНАЛИЗ ДАТАСЕТА 02\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db5561c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ДАТАСЕТ 02: Нелинейная структура + выбросы + шумовой признак\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Загрузка данных\n",
    "df2 = load_dataset(\"S07-hw-dataset-02.csv\")\n",
    "df2 = basic_eda(df2, \"Dataset 02\")\n",
    "\n",
    "# Препроцессинг\n",
    "X2_scaled, X2_raw, sample_ids2 = preprocess_data(df2, \"Dataset 02\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa01ef5a",
   "metadata": {},
   "source": [
    "# 3.1 KMeans для Dataset 02\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bd4430",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KMEANS для Dataset 02\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Подбор оптимального k\n",
    "best_k2, best_sil2 = plot_silhouette_vs_k(X2_scaled, \"Dataset 02\", k_range=(2, 15), save=True)\n",
    "\n",
    "# Лучшая модель KMeans\n",
    "kmeans2 = KMeans(n_clusters=best_k2, random_state=RANDOM_STATE, n_init=10)\n",
    "labels_kmeans2 = kmeans2.fit_predict(X2_scaled)\n",
    "\n",
    "# Оценка\n",
    "metrics_kmeans2 = evaluate_clustering(X2_scaled, labels_kmeans2, \"Dataset 02\", \"KMeans\")\n",
    "\n",
    "# Визуализация\n",
    "var_exp2_kmeans = visualize_clusters_pca(X2_scaled, labels_kmeans2, \n",
    "                                        \"Dataset 02\", \"KMeans\", \n",
    "                                        f\"k={best_k2}\", save=True)\n",
    "\n",
    "# Сохранение результатов\n",
    "save_results(\"dataset02\", \"kmeans\", \n",
    "            {\"n_clusters\": best_k2, \"random_state\": RANDOM_STATE},\n",
    "            metrics_kmeans2, labels_kmeans2, sample_ids2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c904847",
   "metadata": {},
   "source": [
    "# 3.2 Agglomerative Clustering для Dataset 02\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c06d7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"AGGLOMERATIVE CLUSTERING для Dataset 02\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Тестируем разные linkage\n",
    "linkage_methods = ['ward', 'complete', 'average', 'single']\n",
    "best_linkage2 = None\n",
    "best_silhouette2_agg = -1\n",
    "best_labels2_agg = None\n",
    "\n",
    "for linkage in linkage_methods:\n",
    "    # Используем тот же k, что и у KMeans для сравнения\n",
    "    agg = AgglomerativeClustering(n_clusters=best_k2, linkage=linkage)\n",
    "    labels = agg.fit_predict(X2_scaled)\n",
    "    \n",
    "    if len(np.unique(labels)) > 1:\n",
    "        silhouette = silhouette_score(X2_scaled, labels)\n",
    "    else:\n",
    "        silhouette = -1\n",
    "    \n",
    "    if silhouette > best_silhouette2_agg:\n",
    "        best_silhouette2_agg = silhouette\n",
    "        best_linkage2 = linkage\n",
    "        best_labels2_agg = labels\n",
    "\n",
    "print(f\"Лучший linkage: {best_linkage2}\")\n",
    "print(f\"Лучший silhouette: {best_silhouette2_agg:.3f}\")\n",
    "\n",
    "# Лучшая модель Agglomerative\n",
    "agg2 = AgglomerativeClustering(n_clusters=best_k2, linkage=best_linkage2)\n",
    "labels_agg2 = agg2.fit_predict(X2_scaled)\n",
    "\n",
    "metrics_agg2 = evaluate_clustering(X2_scaled, labels_agg2, \"Dataset 02\", \"Agglomerative\")\n",
    "\n",
    "# Визуализация\n",
    "var_exp2_agg = visualize_clusters_pca(X2_scaled, labels_agg2, \n",
    "                                     \"Dataset 02\", \"Agglomerative\", \n",
    "                                     f\"k={best_k2}, linkage={best_linkage2}\", \n",
    "                                     save=True)\n",
    "\n",
    "# Сохранение результатов\n",
    "save_results(\"dataset02\", \"agglomerative\", \n",
    "            {\"n_clusters\": best_k2, \"linkage\": best_linkage2},\n",
    "            metrics_agg2, labels_agg2, sample_ids2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173ce008",
   "metadata": {},
   "source": [
    "# 4. АНАЛИЗ ДАТАСЕТА 03\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfd0736",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ДАТАСЕТ 03: Кластеры разной плотности + фоновый шум\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Загрузка данных\n",
    "df3 = load_dataset(\"S07-hw-dataset-03.csv\")\n",
    "df3 = basic_eda(df3, \"Dataset 03\")\n",
    "\n",
    "# Препроцессинг\n",
    "X3_scaled, X3_raw, sample_ids3 = preprocess_data(df3, \"Dataset 03\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3822ee42",
   "metadata": {},
   "source": [
    "# 4.1 KMeans для Dataset 03\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26e2832",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KMEANS для Dataset 03\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Подбор оптимального k\n",
    "best_k3, best_sil3 = plot_silhouette_vs_k(X3_scaled, \"Dataset 03\", k_range=(2, 15), save=True)\n",
    "\n",
    "# Лучшая модель KMeans\n",
    "kmeans3 = KMeans(n_clusters=best_k3, random_state=RANDOM_STATE, n_init=10)\n",
    "labels_kmeans3 = kmeans3.fit_predict(X3_scaled)\n",
    "\n",
    "# Оценка\n",
    "metrics_kmeans3 = evaluate_clustering(X3_scaled, labels_kmeans3, \"Dataset 03\", \"KMeans\")\n",
    "\n",
    "# Визуализация\n",
    "var_exp3_kmeans = visualize_clusters_pca(X3_scaled, labels_kmeans3, \n",
    "                                        \"Dataset 03\", \"KMeans\", \n",
    "                                        f\"k={best_k3}\", save=True)\n",
    "\n",
    "# Сохранение результатов\n",
    "save_results(\"dataset03\", \"kmeans\", \n",
    "            {\"n_clusters\": best_k3, \"random_state\": RANDOM_STATE},\n",
    "            metrics_kmeans3, labels_kmeans3, sample_ids3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576a823e",
   "metadata": {},
   "source": [
    "# 4.2 DBSCAN для Dataset 03 (хорошо для разной плотности)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90da84d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DBSCAN для Dataset 03\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Для датасета с разной плотностью DBSCAN может быть лучше\n",
    "# Используем k-distance plot для выбора eps\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# k-distance plot\n",
    "neighbors = NearestNeighbors(n_neighbors=5)\n",
    "neighbors_fit = neighbors.fit(X3_scaled)\n",
    "distances, indices = neighbors_fit.kneighbors(X3_scaled)\n",
    "\n",
    "# Сортируем расстояния до 5-го соседа\n",
    "k_distance = np.sort(distances[:, 4])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(len(k_distance)), k_distance)\n",
    "plt.xlabel('Points sorted by distance to 5th nearest neighbor')\n",
    "plt.ylabel('5th nearest neighbor distance')\n",
    "plt.title('Dataset 03 - k-distance plot for DBSCAN (k=5)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Отмечаем возможные значения eps\n",
    "for eps in [0.5, 1.0, 1.5, 2.0]:\n",
    "    plt.axhline(y=eps, color='r', linestyle='--', alpha=0.5)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIGURES_PATH, \"dataset03_k_distance_plot.png\"), dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Подбор параметров DBSCAN\n",
    "eps_values = [0.8, 1.0, 1.2, 1.5, 2.0]\n",
    "min_samples_values = [3, 5, 7]\n",
    "\n",
    "best_eps3 = None\n",
    "best_min_samples3 = None\n",
    "best_silhouette3_db = -1\n",
    "best_labels3_db = None\n",
    "\n",
    "for eps in eps_values:\n",
    "    for min_samples in min_samples_values:\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        labels = dbscan.fit_predict(X3_scaled)\n",
    "        \n",
    "        # Исключаем шум для вычисления метрик\n",
    "        non_noise_mask = labels != -1\n",
    "        if non_noise_mask.sum() > 1 and len(np.unique(labels[non_noise_mask])) > 1:\n",
    "            silhouette = silhouette_score(X3_scaled[non_noise_mask], labels[non_noise_mask])\n",
    "        else:\n",
    "            silhouette = -1\n",
    "        \n",
    "        if silhouette > best_silhouette3_db:\n",
    "            best_silhouette3_db = silhouette\n",
    "            best_eps3 = eps\n",
    "            best_min_samples3 = min_samples\n",
    "            best_labels3_db = labels\n",
    "\n",
    "print(f\"Лучшие параметры DBSCAN: eps={best_eps3}, min_samples={best_min_samples3}\")\n",
    "print(f\"Лучший silhouette: {best_silhouette3_db:.3f}\")\n",
    "\n",
    "# Лучшая модель DBSCAN\n",
    "dbscan3 = DBSCAN(eps=best_eps3, min_samples=best_min_samples3)\n",
    "labels_dbscan3 = dbscan3.fit_predict(X3_scaled)\n",
    "\n",
    "metrics_dbscan3 = evaluate_clustering(X3_scaled, labels_dbscan3, \"Dataset 03\", \"DBSCAN\")\n",
    "\n",
    "# Визуализация\n",
    "var_exp3_dbscan = visualize_clusters_pca(X3_scaled, labels_dbscan3, \n",
    "                                        \"Dataset 03\", \"DBSCAN\", \n",
    "                                        f\"eps={best_eps3}, min_samples={best_min_samples3}\", \n",
    "                                        save=True)\n",
    "\n",
    "# Сохранение результатов\n",
    "save_results(\"dataset03\", \"dbscan\", \n",
    "            {\"eps\": best_eps3, \"min_samples\": best_min_samples3},\n",
    "            metrics_dbscan3, labels_dbscan3, sample_ids3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787b5507",
   "metadata": {},
   "source": [
    "# 5. ПРОВЕРКА УСТОЙЧИВОСТИ (для Dataset 01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074f5488",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ПРОВЕРКА УСТОЙЧИВОСТИ KMeans для Dataset 01\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Проверяем устойчивость KMeans с разными random_state\n",
    "n_runs = 5\n",
    "ari_scores = []\n",
    "\n",
    "# Первый запуск как базовый\n",
    "kmeans_base = KMeans(n_clusters=best_k1, random_state=RANDOM_STATE, n_init=10)\n",
    "labels_base = kmeans_base.fit_predict(X1_scaled)\n",
    "\n",
    "for i in range(n_runs):\n",
    "    kmeans_temp = KMeans(n_clusters=best_k1, random_state=i*100, n_init=10)\n",
    "    labels_temp = kmeans_temp.fit_predict(X1_scaled)\n",
    "    \n",
    "    # Вычисляем ARI между базовой и текущей кластеризацией\n",
    "    ari = adjusted_rand_score(labels_base, labels_temp)\n",
    "    ari_scores.append(ari)\n",
    "    \n",
    "    print(f\"Запуск {i+1} (random_state={i*100}): ARI = {ari:.4f}\")\n",
    "\n",
    "print(f\"\\nСредний ARI: {np.mean(ari_scores):.4f}\")\n",
    "print(f\"Стандартное отклонение ARI: {np.std(ari_scores):.4f}\")\n",
    "\n",
    "if np.mean(ari_scores) > 0.9:\n",
    "    print(\"Кластеризация УСТОЙЧИВА (высокое согласие между запусками)\")\n",
    "elif np.mean(ari_scores) > 0.7:\n",
    "    print(\"Кластеризация УМЕРЕННО УСТОЙЧИВА\")\n",
    "else:\n",
    "    print(\"Кластеризация НЕУСТОЙЧИВА\")\n",
    "\n",
    "# Визуализация результатов разных запусков\n",
    "fig, axes = plt.subplots(1, n_runs, figsize=(20, 4))\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    kmeans_temp = KMeans(n_clusters=best_k1, random_state=i*100, n_init=10)\n",
    "    labels_temp = kmeans_temp.fit_predict(X1_scaled)\n",
    "    \n",
    "    # PCA для визуализации\n",
    "    pca = PCA(n_components=2, random_state=RANDOM_STATE)\n",
    "    X_pca = pca.fit_transform(X1_scaled)\n",
    "    \n",
    "    scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], c=labels_temp, cmap='tab20', s=30, alpha=0.8)\n",
    "    ax.set_title(f'Run {i+1}, seed={i*100}\\nARI={ari_scores[i]:.3f}')\n",
    "    ax.set_xlabel('PC1')\n",
    "    ax.set_ylabel('PC2')\n",
    "\n",
    "plt.suptitle(f'Устойчивость KMeans для Dataset 01 (k={best_k1})', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIGURES_PATH, \"dataset01_stability_kmeans.png\"), dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7983e9d",
   "metadata": {},
   "source": [
    "# 6. СВОДКА РЕЗУЛЬТАТОВ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c1f3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"СВОДКА РЕЗУЛЬТАТОВ ПО ВСЕМ ДАТАСЕТАМ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Собираем все метрики\n",
    "summary = {\n",
    "    'dataset01': {\n",
    "        'kmeans': {\n",
    "            'params': {'n_clusters': best_k1, 'random_state': RANDOM_STATE},\n",
    "            'metrics': metrics_kmeans1\n",
    "        },\n",
    "        'dbscan': {\n",
    "            'params': {'eps': best_eps1, 'min_samples': best_min_samples1},\n",
    "            'metrics': metrics_dbscan1\n",
    "        }\n",
    "    },\n",
    "    'dataset02': {\n",
    "        'kmeans': {\n",
    "            'params': {'n_clusters': best_k2, 'random_state': RANDOM_STATE},\n",
    "            'metrics': metrics_kmeans2\n",
    "        },\n",
    "        'agglomerative': {\n",
    "            'params': {'n_clusters': best_k2, 'linkage': best_linkage2},\n",
    "            'metrics': metrics_agg2\n",
    "        }\n",
    "    },\n",
    "    'dataset03': {\n",
    "        'kmeans': {\n",
    "            'params': {'n_clusters': best_k3, 'random_state': RANDOM_STATE},\n",
    "            'metrics': metrics_kmeans3\n",
    "        },\n",
    "        'dbscan': {\n",
    "            'params': {'eps': best_eps3, 'min_samples': best_min_samples3},\n",
    "            'metrics': metrics_dbscan3\n",
    "        }\n",
    "    },\n",
    "    'stability_check': {\n",
    "        'dataset': 'dataset01',\n",
    "        'method': 'kmeans',\n",
    "        'n_runs': n_runs,\n",
    "        'mean_ari': float(np.mean(ari_scores)),\n",
    "        'std_ari': float(np.std(ari_scores))\n",
    "    }\n",
    "}\n",
    "\n",
    "# Сохраняем сводку в JSON\n",
    "summary_path = os.path.join(ARTIFACTS_PATH, 'metrics_summary.json')\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "print(f\"Сводка метрик сохранена: {summary_path}\")\n",
    "\n",
    "# Сохраняем лучшие конфигурации\n",
    "best_configs = {}\n",
    "for dataset, models in summary.items():\n",
    "    if dataset != 'stability_check':\n",
    "        # Выбираем лучший метод по silhouette\n",
    "        best_model = None\n",
    "        best_score = -1\n",
    "        \n",
    "        for model_name, data in models.items():\n",
    "            silhouette = data['metrics'].get('silhouette', -1)\n",
    "            if silhouette is not None and silhouette > best_score:\n",
    "                best_score = silhouette\n",
    "                best_model = model_name\n",
    "        \n",
    "        if best_model:\n",
    "            best_configs[dataset] = {\n",
    "                'best_model': best_model,\n",
    "                'params': models[best_model]['params'],\n",
    "                'silhouette': best_score\n",
    "            }\n",
    "\n",
    "best_configs_path = os.path.join(ARTIFACTS_PATH, 'best_configs.json')\n",
    "with open(best_configs_path, 'w') as f:\n",
    "    json.dump(best_configs, f, indent=2)\n",
    "print(f\"Лучшие конфигурации сохранены: {best_configs_path}\")\n",
    "\n",
    "# Выводим таблицу сравнения\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ТАБЛИЦА СРАВНЕНИЯ МЕТОДОВ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for dataset in ['dataset01', 'dataset02', 'dataset03']:\n",
    "    print(f\"\\n{dataset.upper()}:\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    if dataset in summary:\n",
    "        for model_name, data in summary[dataset].items():\n",
    "            metrics = data['metrics']\n",
    "            print(f\"  {model_name.upper():15} | \", end=\"\")\n",
    "            print(f\"Clusters: {metrics.get('n_clusters', 'N/A'):3} | \", end=\"\")\n",
    "            \n",
    "            if model_name == 'dbscan':\n",
    "                print(f\"Noise: {metrics.get('noise_ratio', 0):.1%} | \", end=\"\")\n",
    "            \n",
    "            silhouette = metrics.get('silhouette')\n",
    "            if silhouette is not None:\n",
    "                print(f\"Silhouette: {silhouette:.3f}\")\n",
    "            else:\n",
    "                print(f\"Silhouette: N/A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dc3f09",
   "metadata": {},
   "source": [
    "# 7. ВЫВОДЫ ПО ДАТАСЕТАМ\n",
    "\n",
    "1. DATASET 01 (числовые признаки в разных шкалах):\n",
    "   - Масштабирование было критически важно\n",
    "   - KMeans показал хорошие результаты при k=2 (silhouette=0.522)\n",
    "   - DBSCAN с параметрами eps=0.5, min_samples=5 дал аналогичный результат\n",
    "   - Оба метода выделили 2 кластера, результаты устойчивы (ARI=1.0)\n",
    "   - Лучший метод: KMeans (проще интерпретировать, нет шума)\n",
    "\n",
    "2. DATASET 02 (нелинейная структура + шум):\n",
    "   - KMeans показал низкое качество (silhouette=0.307) из-за нелинейной формы\n",
    "   - Agglomerative с linkage='single' значительно лучше (silhouette=0.521)\n",
    "   - DB index 0.342 vs 1.323 подтверждает превосходство иерархической кластеризации\n",
    "   - Single linkage лучше всего справился с нелинейными кластерами\n",
    "   - Лучший метод: AgglomerativeClustering с linkage='single'\n",
    "\n",
    "3. DATASET 03 (разная плотность + шум):\n",
    "   - KMeans выделил 3 кластера с silhouette=0.316\n",
    "   - DBSCAN с eps=0.8, min_samples=3 показал лучше (silhouette=0.373)\n",
    "   - DBSCAN корректно выделил шум (0.15% точек)\n",
    "   - DB index 0.551 vs 1.158 показывает лучшее разделение DBSCAN\n",
    "   - Лучший метод: DBSCAN (лучше адаптируется к разной плотности)\n",
    "\n",
    "4. ОБЩИЕ НАБЛЮДЕНИЯ:\n",
    "   - Масштабирование обязательно для distance-based методов\n",
    "   - Silhouette score хорош для подбора k в KMeans\n",
    "   - DBSCAN чувствителен к параметрам eps и min_samples\n",
    "   - k-distance plot полезен для выбора eps в DBSCAN\n",
    "   - Для нелинейных кластеров иерархическая кластеризация лучше KMeans\n",
    "   - Для кластеров разной плотности DBSCAN предпочтительнее\n",
    "   - Визуализация в PCA помогает интерпретировать результаты"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
